

import os
from pathlib import Path
import json
import joblib
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# -------- CONFIG: chosen features (6 from allowed 9) ----------
NUM_FEATURES = ["OverallQual", "GrLivArea", "TotalBsmtSF", "GarageCars", "YearBuilt"]
CAT_FEATURES = ["Neighborhood"]
ALL_FEATURES = NUM_FEATURES + CAT_FEATURES
TARGET = "SalePrice"

# Paths
ROOT = Path(__file__).resolve().parent
DATA_PATH = ROOT / "data" / "train.csv"
MODEL_OUT = ROOT / "house_price_model.pkl"
NEIGHBOR_OUT = ROOT / "neighborhood_categories.json"

def load_data(path: Path) -> pd.DataFrame:
    if not path.exists():
        raise FileNotFoundError(f"Place the Kaggle 'train.csv' at: {path}")
    return pd.read_csv(path)

def build_pipeline():
    """
    Build a preprocessing + model pipeline:
    - numeric: median imputer -> StandardScaler
    - categorical: most_frequent imputer -> OneHotEncoder(handle_unknown='ignore')
    - regressor: RandomForestRegressor
    """
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=False))
    ])

    preprocessor = ColumnTransformer(transformers=[
        ("num", numeric_transformer, NUM_FEATURES),
        ("cat", categorical_transformer, CAT_FEATURES)
    ])

    pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("regressor", RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))
    ])

    return pipeline

def evaluate_model(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    return mae, mse, rmse, r2

def main():
    print("Loading data...")
    df = load_data(DATA_PATH)

    # Keep only the allowed features + target
    missing_cols = [c for c in ALL_FEATURES + [TARGET] if c not in df.columns]
    if missing_cols:
        raise ValueError(f"Expected columns missing in CSV: {missing_cols}")

    df_model = df[ALL_FEATURES + [TARGET]].copy()
    # Show simple info
    print("Dataset shape (rows, cols):", df_model.shape)
    print("Sample rows:\n", df_model.head(3))

    # Split X/y
    X = df_model[ALL_FEATURES]
    y = df_model[TARGET].values

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
    print(f"Training samples: {len(X_train)}, Test samples: {len(X_test)}")

    # Build and fit pipeline
    pipeline = build_pipeline()
    print("Fitting pipeline (this can take a minute)...")
    pipeline.fit(X_train, y_train)

    # Evaluate
    print("Predicting on test set...")
    y_pred = pipeline.predict(X_test)
    mae, mse, rmse, r2 = evaluate_model(y_test, y_pred)
    print("Evaluation metrics on test set:")
    print(f" MAE : {mae:,.2f}")
    print(f" MSE : {mse:,.2f}")
    print(f" RMSE: {rmse:,.2f}")
    print(f" R2  : {r2:.4f}")

    # Persist pipeline
    print(f"Saving pipeline to: {MODEL_OUT}")
    joblib.dump(pipeline, MODEL_OUT)

    # Extract Neighborhood categories (for populating form)
    try:
        pre = pipeline.named_steps["preprocessor"]
        cat_pipe = pre.named_transformers_["cat"]
        onehot = cat_pipe.named_steps["onehot"]
        neighborhoods = onehot.categories_[0].tolist()
    except Exception as e:
        print("Warning: could not extract categories from pipeline:", e)
        neighborhoods = sorted(X["Neighborhood"].dropna().unique().tolist())

    with open(NEIGHBOR_OUT, "w", encoding="utf-8") as f:
        json.dump(neighborhoods, f, indent=2)
    print(f"Saved neighborhood list to: {NEIGHBOR_OUT}")
    print("Model training and saving complete.")

if __name__ == "__main__":
    main()
